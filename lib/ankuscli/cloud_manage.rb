=begin
  Class to manage cloud instances, create/delete
=end

module Ankuscli
  require 'benchmark'
  require 'thread'
  require 'erb'
  require 'tempfile'
  class Cloud
    # Create a new Cloud class object
    # @param [String] provider => Cloud service provider; aws|rackspace
    # @param [Hash] parsed_config => Configuration that has been already parsed from cloud_configuration file
    # @param [Hash] cloud_credentials => Credentials configurations
    #     if aws: cloud_credentials => { aws_access_id: '', aws_secret_key: '', aws_machine_type: 'm1.large', aws_region: 'us-west-1', aws_key: 'ankuscli' }
    #     if rackspace: cloud_credentials => { rackspace_username: '', rackspace_api_key: '', rackspace_instance_type: '', rackspace_ssh_key: '~/.ssh/id_rsa.pub' }
    # @param [Integer] thread_pool_size => number of threads to use to perform instance creation, volume attachements
    # @param [Boolean] debug => if enabled will print more info to stdout
    def initialize(provider, parsed_config, cloud_credentials, thread_pool_size = 10, debug = false)
      @provider = provider || parsed_config['cloud_platform']
      @parsed_hash = parsed_config
      @credentials = cloud_credentials
      @debug = debug
      @thread_pool_size = thread_pool_size
      raise unless @credentials.is_a?(Hash)
    end

    # Parse cloud_configuration; create instances and return instance mappings
    # @return [Hash] nodes:
    #   for aws cloud, nodes: { 'tag' => [public_dns_name, private_dns_name], 'tag' => [public_dns_name, private_dns_name], ... }
    #   for rackspace, nodes: { 'tag' => [public_ip_address, fqdn], 'tag' => [public_ip_address, fqdn], ... }
    def create_instances
      num_of_slaves = @parsed_hash['slave_nodes_count']
      num_of_zks = @parsed_hash['zookeeper_quorum_count']
      cloud_os_type = @parsed_hash['cloud_os_type']
      slave_nodes_disk_size = @parsed_hash['slave_nodes_storage_capacity'] || 0
      nodes_created = {}
      nodes_to_create = {}

      if @provider == 'aws'
        #calculate number of disks and their size
        if slave_nodes_disk_size.nil? or slave_nodes_disk_size.to_i == 0
          #assume user do not want any extra volumes
          @volumes_count = 0
          @volume_size = 0
        else
          # user wants extra volumes
          @volume_count = 4
          @volume_size = slave_nodes_disk_size / @volume_count
        end
        #create controller
        nodes_to_create['controller'] = { :os_type => cloud_os_type, :volumes => 0, :volume_size => 50 }
        # if ha is enabled
        if @parsed_hash['hadoop_ha'] == 'enabled'
          nodes_to_create['namenode1'] = { :os_type => cloud_os_type, :volumes => 0, :volume_size => 50 }
          nodes_to_create['namenode2'] = { :os_type => cloud_os_type, :volumes => 0, :volume_size => 50 }
          nodes_to_create['jobtracker'] = { :os_type => cloud_os_type, :volumes => 0, :volume_size => 50 }
          num_of_zks.times do |i|
            nodes_to_create["zookeeper#{i+1}"] = { :os_type => cloud_os_type, :volumes => 0, :volume_size => 50 }
          end
          num_of_slaves.times do |i|
            nodes_to_create["slaves#{i+1}"] = { :os_type => cloud_os_type, :volumes => @volume_count, :volume_size => @volume_size }
          end
        else
          # if ha is not enabled
          nodes_to_create['namenode'] = { :os_type => cloud_os_type, :volumes => 0, :volume_size => 50 }
          nodes_to_create['jobtracker'] = { :os_type => cloud_os_type, :volumes => 0, :volume_size => 50 } #JT and SNN
          num_of_slaves.times do |i|
            nodes_to_create["slaves#{i+1}"] = { :os_type => cloud_os_type, :volumes => @volume_count, :volume_size => @volume_size }
          end
        end
        nodes_created = create_on_aws(nodes_to_create, @credentials, @thread_pool_size)
      elsif @provider == 'rackspace'
        #TODO
      end
      #returns parse nodes hash
      nodes_created
    end

    # Modifies the original parsed_config hash to look more like the local install mode
    # @param [Hash] parsed_hash => original parsed hash generated from configuration file
    # @param [Hash] nodes_hash => nodes hash generated by create_on_aws method in this class
    # @return if rackspace [Hash] parsed_hash => which can be used same as local install_mode
    #         if aws [Hash, Hash] parsed_hash, parsed_internal_ips => which can be used same as local install_mode
    def modify_config_hash(parsed_hash, nodes_hash)
      if @provider == 'aws'
        parsed_hash_internal_ips = Marshal.load(Marshal.dump(parsed_hash))
        #things to add back to parsed_hash:
        # root_ssh_key:
        # controller:
        # hadoop_namenode: []
        # hadoop_secondarynamenode: if hadoop_ha == 'disabled'
        # zookeeper_quorum: []
        # journal_quorum: []
        # mapreduce['master']:
        # slave_nodes: []
        # hbase_master: [] if hbase_install == enabled
        parsed_hash['root_ssh_key'] = File.expand_path('~/.ssh') + '/' + @parsed_hash['cloud_credentials']['aws_key']
        parsed_hash['controller'] = nodes_hash['controller'].first
        parsed_hash['hadoop_namenode'] = nodes_hash.map { |k,v| v.first if k =~ /namenode/ }.compact
        parsed_hash['mapreduce']['master'] = nodes_hash['jobtracker'].first
        parsed_hash['hadoop_secondarynamenode'] = nodes_hash['jobtracker'].first if parsed_hash['hadoop_ha'] == 'disabled'
        parsed_hash['slave_nodes'] = nodes_hash.map { |k,v| v.first if k =~ /slaves/ }.compact
        parsed_hash['zookeeper_quorum'] = nodes_hash.map { |k,v| v.first if k =~ /zookeeper/ }.compact if parsed_hash['hadoop_ha'] == 'enabled' or parsed_hash['hbase_install'] == 'enabled'
        parsed_hash['journal_quorum'] = nodes_hash.map { |k,v| v.first if k =~ /zookeeper/ }.compact if parsed_hash['hadoop_ha'] == 'enabled'
        parsed_hash['hbase_master'] = nodes_hash.map { |k,v| v.first if k =~ /hbasemaster/ }.compact if parsed_hash['hbase_install'] == 'enabled'

        #hash with internal ips
        parsed_hash_internal_ips['root_ssh_key'] = File.expand_path('~/.ssh') + '/' + @parsed_hash['cloud_credentials']['aws_key']
        parsed_hash_internal_ips['controller'] = nodes_hash['controller'].last
        parsed_hash_internal_ips['hadoop_namenode'] = nodes_hash.map { |k,v| v.last if k =~ /namenode/ }.compact
        parsed_hash_internal_ips['mapreduce']['master'] = nodes_hash['jobtracker'].last
        parsed_hash_internal_ips['hadoop_secondarynamenode'] = nodes_hash['jobtracker'].last if parsed_hash['hadoop_ha'] == 'disabled'
        parsed_hash_internal_ips['slave_nodes'] = nodes_hash.map { |k,v| v.last if k =~ /slaves/ }.compact
        parsed_hash_internal_ips['zookeeper_quorum'] = nodes_hash.map { |k,v| v.last if k =~ /zookeeper/ }.compact if parsed_hash['hadoop_ha'] == 'enabled' or parsed_hash['hbase_install'] == 'enabled'
        parsed_hash_internal_ips['journal_quorum'] = nodes_hash.map { |k,v| v.last if k =~ /zookeeper/ }.compact if parsed_hash['hadoop_ha'] == 'enabled'
        parsed_hash_internal_ips['hbase_master'] = nodes_hash.map { |k,v| v.last if k =~ /hbasemaster/ }.compact if parsed_hash['hbase_install'] == 'enabled'
        return parsed_hash, parsed_hash_internal_ips
      elsif @provider == 'rackspace'
        #TODO
      end
    end

    # Create servers on aws using Ankuscli::Aws
    # @param [Hash] nodes_to_create => hash of nodes to create with their info as shown below
    #      { 'node_tag' => { :os_type => cloud_os_type, :volumes => 2, :volume_size => 50 }, ... }
    # @param [Hash] credentials: {  aws_access_id: '', aws_secret_key: '', aws_machine_type: 'm1.large', aws_region: 'us-west-1', aws_key: 'ankuscli'}
    # @param [Integer] thread_pool_size => size of the thread pool
    # @return [Hash] results => { 'instance_tag' => [public_dns_name, private_dns_name], ... }
    def create_on_aws(nodes_to_create, credentials, thread_pool_size)
      #defaults
      threads_pool = Ankuscli::ThreadPool.new(thread_pool_size)
      key = credentials['aws_key'] || 'ankuscli'
      groups = credentials['aws_sec_groups'] || %w(ankuscli)
      flavor_id = credentials['aws_machine_type'] || 'm1.large'
      aws = Ankuscli::Aws.new(credentials['aws_access_id'], credentials['aws_secret_key'], credentials['aws_region'])
      conn = aws.create_connection
      results = {}

      if aws.valid_connection?(conn)
        puts 'successfully connected to aws'.green if @debug
      else
        puts '[Error]'.red + ' failed connecting to aws'
        exit 1
      end

      #a thread pool for creating servers
      puts 'Creating servers with tags: ' + "#{nodes_to_create.keys.join(',')}".blue
      #hash to store server object to tag mapping { tag => server_obj }, used for attaching volumes
      server_objects = {}
      nodes_to_create.each do |tag, info|
        server_objects[tag] = aws.create_server!(conn,
                                                 tag,
                                                 :key => key,
                                                 :groups => groups,
                                                 :flavor_id => flavor_id,
                                                 :os_type => info[:os_type],
                                                 :num_of_vols => info[:volumes],
                                                 :vol_size => info[:volume_size]
        )
      end
      #wait for servers to get created (:state => running)
      aws.wait_for_servers(server_objects.values)
      #wait for the boot to complete
      aws.complete_wait(server_objects, info[:os_type]) #TODO: this method is taking forever, find another way to make sure volumes are properly mounted
      #build the return string
      nodes_to_create.each do |tag, info|
        # fill in return hash
        results[tag] = [ server_objects[tag].dns_name, server_objects[tag].private_dns_name ]
      end
      puts 'Partitioning/Formatting attached volumes'.blue
      #parition and format attached disks using thread pool
      nodes_to_create.each do |tag, info|
        threads_pool.schedule do
          #build partition script
          partition_script = gen_partition_script(info[:volumes])
          tempfile = Tempfile.new('partition')
          tempfile.write(partition_script)
          tempfile.close
          # wait for the server to be ssh'able
          Ankuscli::SshUtils.wait_for_ssh(server_objects[tag].dns_name, 'root', File.expand_path('~/.ssh') + "/#{key}")
          # upload and execute the partition script on the remote machine
          SshUtils.upload!(tempfile.path, '/tmp', server_objects[tag].dns_name, @ssh_user, @ssh_key)
          Ankuscli::SshUtils.execute_ssh!(
              "chmod +x /tmp/#{File.basename(tempfile.path)} && sudo sh -c '/tmp/#{File.basename(tempfile.path)} > /var/log/bootstrap_volumes.log 2>&1'",
              server_objects[tag].dns_name,
              @ssh_user,
              File.expand_path('~/.ssh') + "/#{key}",
              22,
              false)
          tempfile.unlink #delete the tempfile
        end
      end
      threads_pool.shutdown

      puts '[Debug]: Finished creating and attaching volumes' if @debug

      # Test data output from this method
      #results = {
      #    'controller' => ['ec2-54-215-78-76.us-west-1.compute.amazonaws.com', 'ip-10-197-0-31.us-west-1.compute.internal'],
      #    'namenode1' => ['ec2-54-215-91-134.us-west-1.compute.amazonaws.com', 'ip-10-197-53-160.us-west-1.compute.internal'],
      #    'namenode2' => ['ec2-54-215-70-117.us-west-1.compute.amazonaws.com', 'ip-10-197-47-222.us-west-1.compute.internal'],
      #    'jobtracker' => ['ec2-54-241-47-231.us-west-1.compute.amazonaws.com', 'ip-10-197-27-15.us-west-1.compute.internal'],
      #    'zookeeper1' => ['ec2-54-215-121-39.us-west-1.compute.amazonaws.com', 'ip-10-197-5-46.us-west-1.compute.internal'],
      #    'slaves1' => ['ec2-54-215-73-98.us-west-1.compute.amazonaws.com', 'ip-10-196-21-27.us-west-1.compute.internal'],
      #    'slaves2' => ['ec2-54-215-73-12.us-west-1.compute.amazonaws.com', 'ip-10-196-42-106.us-west-1.compute.internal'],
      #    'slaves3' => ['ec2-54-215-71-122.us-west-1.compute.amazonaws.com', 'ip-10-196-19-36.us-west-1.compute.internal']
      #}
      results
    end

    # @return [Hash] results => { 'instance_tag' => [public_ip_address, fqdn], ... }
    def create_on_rackspace(credentials, options = {})
      #instances_count = options[:count] || 1
      #server_tag = options[:tag] || 'ankuscli'
      #server_name = options[:machine_name] || 'test.ankuscli.com'
      #os_type = options[:os_type] || 'CentOS'
      #volume_size = options[:volume_size] || 0
      #api_key = credentials['rackspace_api_key']
      #username = credentials['rackspace_username']
      #machine_type = credentials['rackspace_instance_type']
      #ssh_key_path = credentials['rackspace_ssh_key']
    end

    # Builds and returns a shell script for partitioning and resizing volumes
    # @param [Integer] number_of_volumes => number of volumes to partition & mount, this number is used to grep the
    #                                       value from /proc/partitions
    # @return [ERB] build out shell script
    def gen_partition_script(number_of_volumes)
      template = <<-END.gsub(/^ {6}/, '')
      #!/bin/bash
      echo "Resizing the root partition"
      resize2fs /dev/`cat /proc/partitions | awk '/xvd*/ {print $4}' | head -n1`
      NUM_OF_VOLS=<%= number_of_volumes %>
      if [ $NUM_OF_VOLS -ne 0 ]; then
      DEVICES=`cat /proc/partitions | awk '/xvd*/ {print $4}' | tail -n<%= number_of_volumes %>`
      echo "Formatting and mounting initiated"
      count=1
      for dev in $DEVICES; do
      echo "Formatting and mounting $dev"
      fdisk -u /dev/$dev << EOF
      n
      p
      1


      w
      EOF
      mkfs.ext4 /dev/${dev}1
      data_dir=$((count++))
      mkdir -p /data/${data_dir}
      mount /dev/${dev}1 /data/${data_dir}
      done
      fi
      END
      ERB.new(template).result(binding)
    end
  end
end