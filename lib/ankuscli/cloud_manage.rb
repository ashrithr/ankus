=begin
  Class to manage cloud instances, create/delete
=end

module Ankuscli
  class Cloud
    # Create a new Cloud class object
    # @param [String] provider => Cloud service provider; aws|rackspace
    # @param [Hash] parsed_config => Configuration that has been already parsed from cloud_configuration file
    # @param [Hash] cloud_credentials => Credentials configurations
    #     if aws: cloud_credentials => { aws_access_id: '', aws_secret_key: '', aws_machine_type: 'm1.large', aws_region: 'us-west-1', aws_key: 'ankuscli' }
    #     if rackspace: cloud_credentials => { rackspace_username: '', rackspace_api_key: '', rackspace_instance_type: '', rackspace_ssh_key: '~/.ssh/id_rsa.pub' }
    def initialize(provider, parsed_config, cloud_credentials)
      @provider = provider || parsed_config['cloud_platform']
      @parsed_hash = parsed_config
      @credentials = cloud_credentials
      raise unless @credentials.is_a?(Hash)
      @partition_script = DATA.read
    end

    # Parse cloud_configuration; create instances and return instance mappings
    # @return [Hash] nodes:
    #   for aws cloud, nodes: { 'tag' => [public_dns_name, private_dns_name], 'tag' => [public_dns_name, private_dns_name], ... }
    #   for rackspace, nodes: { 'tag' => [public_ip_address, fqdn], 'tag' => [public_ip_address, fqdn], ... }
    def create_instances
      num_of_slaves = @parsed_hash['slave_nodes_count']
      cloud_os_type = @parsed_hash['cloud_os_type']
      slave_nodes_disk_size = @parsed_hash['slave_nodes_storage_capacity'] || 0
      nodes = {}

      num_of_zks = @parsed_hash['zookeeper_quoram_count']
      if @provider == 'aws'
        #calculate number of disks and their size
        if slave_nodes_disk_size.nil?
          #assume user do not want any extra volumes
          @volumes_count = 0
          @volume_size = 0
        else
          # user wants extra volumes
          @volume_count = 4
          @volume_size = slave_nodes_disk_size / @volume_count
        end
        #create controller
        nodes.merge!(create_on_aws(@credentials, :count => 1, :volumes => 2, :volume_size => 50, :os_type => cloud_os_type, :machine_tag => 'controller'))
        if @parsed_hash['hadoop_ha'] == 'enabled'
          # if ha => 2 namenodes, 1 jobtracker, n zookeepers, n slavenodes
          #namenodes
          nodes.merge!(create_on_aws(@credentials, :count => 2, :volumes => 2 ,:volume_size => 50, :os_type => cloud_os_type, :machine_tag => 'namenode'))
          #jobtracker
          nodes.merge!(jobtracker = create_on_aws(@credentials, :count => 1, :volumes => 2, :volume_size => 50, :os_type => cloud_os_type, :machine_tag => 'jobtracker'))
          #zookeepers
          nodes.merge!(zookeepers = create_on_aws(@credentials, :count => num_of_zks.length, :volumes => 2 ,:volume_size => 50, :os_type => cloud_os_type, :machine_tag => 'zookeeper'))
          #slaves
          nodes.merge!(slaves = create_on_aws(@credentials, :count => num_of_zks.length, :volumes => @volume_count ,:volume_size => @volume_size, :os_type => cloud_os_type, :machine_tag => 'slaves'))
        else
          # if non-ha => 1 namenode, 1 jobtracker, n slave_nodes
        end
      elsif @provider == 'rackspace'
        #TODO
      end
      #parse nodes hash
      nodes
    end

    # Modifies the original parsed_config hash to look more like the local install mode
    # @param [Hash] parsed_hash => original parsed hash generated from configuration file
    # @param [Hash] nodes_hash => nodes hash generated by parse_and_create method in this class
    # @return if rackspace [Hash] parsed_hash => which can be used same as local install_mode
    #         if aws [Hash, Hash] parsed_hash, parsed_internal_ips => which can be used same as local install_mode
    def modify_config_hash(parsed_hash, nodes_hash)
      if @provider == 'aws'
        parsed_hash_internal_ips = parsed_hash.clone
        #things to add back to parsed_hash:
        # root_ssh_key:
        # controller:
        # hadoop_namenode: []
        # zookeeper_quoram: []
        # journal_quoram: []
        # mapreduce['master']:
        # slave_nodes: []
        # hbase_master: [] if hbase_install == enabled
        parsed_hash['root_ssh_key'] = File.expand_path('~/.ssh') + '/' + @parsed_hash['cloud_credentials']['aws_key']
        parsed_hash['controller'] = nodes_hash['controller'].first
        parsed_hash['hadoop_namenode'] = nodes_hash.map { |k,v| v.first if k =~ /namenode/ }.compact
        parsed_hash['mapreduce']['master'] = nodes_hash['jobtracker'].first
        parsed_hash['slave_nodes'] = nodes_hash.map { |k,v| v.first if k =~ /slaves/ }.compact
        parsed_hash['zookeeper_quoram'] = nodes_hash.map { |k,v| v.first if k =~ /zookeeper/ }.compact if parsed_hash['hadoop_ha'] == 'enabled' or parsed_hash['hbase_install'] == 'enabled'
        parsed_hash['journal_quoram'] = nodes_hash.map { |k,v| v.first if k =~ /zookeeper/ }.compact if parsed_hash['hadoop_ha'] == 'enabled'
        parsed_hash['hbase_master'] = nodes_hash.map { |k,v| v.first if k =~ /hbasemaster/ }.compact if parsed_hash['hbase_install'] == 'enabled'

        #hash with internal ips
        parsed_hash_internal_ips['root_ssh_key'] = File.expand_path('~/.ssh') + '/' + @parsed_hash['cloud_credentials']['aws_key']
        parsed_hash_internal_ips['controller'] = nodes_hash['controller'].last
        parsed_hash_internal_ips['hadoop_namenode'] = nodes_hash.map { |k,v| v.last if k =~ /namenode/ }.compact
        parsed_hash_internal_ips['mapreduce']['master'] = nodes_hash['jobtracker'].last
        parsed_hash_internal_ips['slave_nodes'] = nodes_hash.map { |k,v| v.last if k =~ /slaves/ }.compact
        parsed_hash_internal_ips['zookeeper_quoram'] = nodes_hash.map { |k,v| v.last if k =~ /zookeeper/ }.compact if parsed_hash['hadoop_ha'] == 'enabled' or parsed_hash['hbase_install'] == 'enabled'
        parsed_hash_internal_ips['journal_quoram'] = nodes_hash.map { |k,v| v.last if k =~ /zookeeper/ }.compact if parsed_hash['hadoop_ha'] == 'enabled'
        parsed_hash_internal_ips['hbase_master'] = nodes_hash.map { |k,v| v.last if k =~ /hbasemaster/ }.compact if parsed_hash['hbase_install'] == 'enabled'

        return parsed_hash, parsed_hash_internal_ips
      elsif @provider == 'rackspace'
        #TODO
      end
    end

    # Create servers on aws using Ankuscli::Aws
    # @param [Hash] credentials: {  aws_access_id: '', aws_secret_key: '', aws_machine_type: 'm1.large', aws_region: 'us-west-1', aws_key: 'ankuscli'}
    # @param [Hash] options: { count: 1, volumes: 4, volume_size: 25 }
    #   @option [Integer] count => number of servers to create
    #   @option [Integer] volumes => number of volumes for each server
    #   @option [Integer] volume_size => size of each volume in GB
    #   @option [String] os_type => type of the os to boot (centos|ubuntu)
    #   @option [String] machine_tag => name for the instance tag
    # @return [Hash] results => { 'instance_tag' => [public_dns_name, private_dns_name], ... }
    def create_on_aws(credentials, options = {})
      #defaults
      instances_count = options[:count] || 1
      tag = options[:machine_tag] || 'ankuscli-instance'
      os_type = options[:os_type] || 'CentOS'
      volume_count = options[:volumes] || 4
      volume_size = options[:volume_size] || 25   #25GB
      key = credentials['aws_key'] || 'ankuscli'
      groups = credentials['aws_sec_groups'] || %w(default)
      flavor_id = credentials['aws_machine_type'] || 'm1.large'

      aws = Ankuscli::Aws.new(credentials['aws_access_id'], credentials['aws_secret_key'], credentials['aws_region'])
      conn = aws.create_connection
      results = {}

      if aws.valid_connection?(conn)
        puts 'successfully connected to aws'.green
      else
        puts '[Error]'.red + ' failed connecting to aws'
        exit 1
      end
      if instances_count == 1
        server = aws.create_server!(conn,
                                    tag,
                                    :key => key,
                                    :groups => groups,
                                    :flavor_id => flavor_id,
                                    :os_type => os_type
        )
        puts "[Debug]: Waiting for aws instance: #{server.id} to get created"
        aws.wait_for_servers(server)
        if volume_count != 0
          puts "[Debug]: Attaching volumes on instance: #{server.id}"
          aws.attach_volumes!(conn, server, volume_count, volume_size)
          #wait until servers become available
          Ankuscli::SshUtils.wait_for_ssh(server.dns_name, 'root', File.expand_path('~/.ssh') + "/#{key}")
          #execute partition script on remote server
          Ankuscli::SshUtils.execute_ssh!(@partition_script, server.dns_name, 'root', File.expand_path('~/.ssh') + "/#{key}", 22, true) #TODO Change it to false
        end
        results[tag] = [server.dns_name, server.private_dns_name]
      elsif instances_count > 1
        servers = aws.create_servers!(conn,
                                      instances_count,
                                      :os_type => os_type,
                                      :key => key,
                                      :groups => groups,
                                      :flavor_id => flavor_id,
                                      :instance_tag => tag
        )
        puts "[Debug]: Waiting for aws instances: #{servers.join(',')} to get created"
        aws.wait_for_servers(servers)
        servers.each_with_index do |server, index|
          results["#{tag}#{index + 1}"] = [server.dns_name, server.private_dns_name]
        end
        #TODO implement multi threading for partitioning and mounting volumes
        servers.each do |server|
          puts "[Debug]: Attaching volumes to instance: #{server.id}"
          aws.attach_volumes!(conn, server, volume_count, volume_size)
          Ankuscli::SshUtils.wait_for_ssh(server.dns_name, 'root', File.expand_path('~/.ssh') + "/#{key}")
          Ankuscli::SshUtils.execute_ssh!(@partition_script, server.dns_name, 'root', File.expand_path('~/.ssh') + "/#{key}", 22, true) #TODO Change it to false
        end
      end
      results
    end

    # @return [Hash] results => { 'instance_tag' => [public_ip_address, fqdn], ... }
    def create_on_rackspace(credentials, options = {})
      instances_count = options[:count] || 1
      server_tag = options[:tag] || 'ankuscli'
      server_name = options[:machine_name] || 'test.ankuscli.com'
      os_type = options[:os_type] || 'CentOS'
      volume_size = options[:volume_size] || 0
      api_key = credentials['rackspace_api_key']
      username = credentials['rackspace_username']
      machine_type = credentials['rackspace_instance_type']
      ssh_key_path = credentials['rackspace_ssh_key']
    end
  end
end

#partition script
__END__
DEVICES=`cat /proc/partitions | awk '/xvd*/ {print $4}' | tail -n4`
echo "Formatting and mounting initiated"
count=1
for dev in $DEVICES; do
echo "Formatting and mounting $dev"
fdisk -u /dev/$dev << EOF
n
p
1


w
EOF
mkfs.ext4 /dev/${dev}1
data_dir=$((count++))
mkdir -p /data/${data_dir}
mount /dev/${dev}1 /data/${data_dir}
done