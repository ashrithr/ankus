---
########################################################
#             AnkusCLI Local Configuration
########################################################

########################################################
# Install mode => Specifies the type of installation
#   Possible values: local | cloud
#     # local: installation will be done on pre-defined machines
#     # cloud: ankus will create machines in the cloud
########################################################
install_mode: 'local'
#root users ssh private key to log into machines
ssh_key: '~/.ssh/ruby-sample-1363113606'
#username used to log into machines, change this if the user is not 'root', ssh_user
#should have password less sudo privileges (you can make a user password-less sudo by adding the user
#to /etc/sudoers file like this `ssh_user  ALL = (ALL) NOPASSWD: ALL` )
ssh_user: 'root'
# controller: hostname of the server which will host puppet master, kerberos server
#  if security is enabled, nagios & ganglia servers if monitoring & altering are enabled.
#  logstash indexer if log_aggregation is enabled this could be local host if running
#  ankuscli from controller machine.
controller: 'controller.ops.cw.com'

########################################################
# Hadoop Cluster Configuration
# hadoop_ha (hadoop high availability configuration):
#   enabled => hadoop ha is enabled with two namenode(s) but without auto failover using zkfc
#   auto    => hadoop ha is enabled with two namenode(s) with auto failover of nodes using zkfc
#   disabled => hadoop ha is not setup and only one namenode will be installed and configured
# hadoop_namenode:
#   fqdn of hosts on which namenode should be installed
#   if hadoop_ha is enabled|auto, pass in two fqdn's
#   if hadoop_ha is disabled, pass in one fqdn
# hadoop_secondarynamenode:
#   if hadoop_ha is disabled pass in a fqdn in which secondarynamenode is configured
# zookeeper_quorum:
#   if hadoop_ha is auto, pass in odd number of fqdn's. zookeepers are nodes which are used for
#   co-ordination for namenode fail-over
# journal_quorum:
#   if hadoop_ha is enabled|auto, pass in odd number of fqdn's. Journal nodes are used to store
#   metadata in high availability configuration
# Note: (zookeepers and journal nodes can co-exist on same machines)
########################################################
hadoop_ha: 'enabled'
hadoop_namenode:
  - 'namenode001.ops.cw.com'
  - 'namenode002.ops.cw.com'
#hadoop_secondarynamenode: 'snn.ops.cw.com'
zookeeper_quorum:
  - 'utils001.ops.cw.com'
  - 'utils002.ops.cw.com'
  - 'utils003.ops.cw.com'
journal_quorum:
  - 'utils001.ops.cw.com'
  - 'utils002.ops.cw.com'
  - 'utils003.ops.cw.com'

########################################################
# MapReduce Configuration
#   type => type of mapreduce framework to setup, supported values mr1 (regular mapreduce) or mr2 (yarn)
#   master => fqdn of host which is responsible for managing mapreduce resources
########################################################
mapreduce:
  type: 'mr1'
  master: 'jobtracker.ops.cw.com'

########################################################
# HBase configuration
# hbase_install => Specify whether to deploy hbase or not
#  if enabled will install hbase
#  if disabled hbase will not be deployed
# hbase_master => if hbase_install is enabled, specify fqdn's of hbase masters to deploy
########################################################
hbase_install: disabled
#hbase_master:
#  - 'hbasemaster001.ops.cw.com'
#  - 'hbasemaster002.ops.cw.com'

########################################################
# Worker Nodes Configuration
#  list of slave nodes on which datanode and tasktracker daemons will be installed. If hbase is enabled
#  region servers are also installed on same instances.
########################################################
slave_nodes:
  - 'worker001.ops.cw.com'
  - 'worker002.ops.cw.com'
  - 'worker003.ops.cw.com'
  - 'worker004.ops.cw.com'
  - 'worker005.ops.cw.com'

########################################################
# Additional Eco-system tools - to install on the cluster
# hive - installs and configures hive, which is data warehouse system for Hadoop (http://hive.apache.org/)
# pig - installs and configures pig, is a platform for analyzing large data sets (http://pig.apache.org/)
# sqoop - installs and configures sqoop, is a tool designed for efficiently transferring bulk data between hadoop &
#         rdbms & mpp (http://sqoop.apache.org/)
# oozie - installs and configures oozie, is a workflow scheduler system to manage Apache Hadoop jobs
#         (http://oozie.apache.org/)
# hue - installs and configures hue, the open source Apache Hadoop UI (http://cloudera.github.io/hue/)
########################################################
hadoop_ecosystem:
  - hive
  - sqoop
  - pig
  - oozie
  - hue

########################################################
# Security Config
# security => if configured sets up kerberos and integrates it with hadoop/hbase and its eco-system
#   possible_values: simple => will not install kerberos
#                    kerberos => will install kerberos
# hadoop_kerberos_realm => if kerberos is enabled, enter a kerberos realm name
# hadoop_kerberos_domain => if kerberos is enabled, enter a kerberos domain name
#   (http://web.mit.edu/kerberos/krb5-1.5/krb5-1.5/doc/krb5-admin/domain_realm.html)
########################################################
security: simple
# if security is kerberos please fill in the below options
#hadoop_kerberos_realm: CW.COM
#hadoop_kerberos_domain: cw.com

########################################################
# Management Configuration
# monitoring (enabled|disabled) => if enabled, ganglia will be setup and configured
# alerting (enabled|disabled) => if enabled, nagios will be setup and configured to monitor
# admin_email => required (if alerting is enabled), email address to send alters (notifications)
# log_aggregation (enabled|disabled) => if enabled, logstash will be installed to aggregate logs from all the servers
########################################################
monitoring: enabled
alerting: enabled
admin_email: 'admin@cw.com'
log_aggregation: enabled