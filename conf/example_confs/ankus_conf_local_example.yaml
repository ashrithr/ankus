---
########################################################
#             Ankus Local Configuration
########################################################
install_mode: 'local'

########################################################
# SSH Configuration
########################################################
# * ssh_key : root users ssh private key to log into machines
# * ssh_user: username used to log into machines, change this if the user is 
#             not 'root', ssh_user should have password less sudo privileges 
#             (you can make a user password-less sudo by adding the user to
#              /etc/sudoers file like this 
#              `ssh_user  ALL = (ALL) NOPASSWD: ALL` 
#             )
########################################################
ssh_key: '~/.ssh/id_rsa'         #<<<<<<<<<<<<<< UPDATE THIS
ssh_user: 'root'                 #<<<<<<<<<<<<<< UPDATE THIS

########################################################
# Controller
########################################################
# * controller: fqdn of the server which will host 
#               - puppet master 
#               - kerberos server (if security is enabled)
#               - nagios server (if alerting is enabled)
#               - ganglia server (if monitoring is enabled)
#               - logstash indexer (if log aggregation is enabled)
#   The value of the controller could be 'localhost' if runing ankus from 
#   controller.
########################################################
controller: 'controller.ops.cw.com'

########################################################
# Hadoop Cluster Configuration
########################################################
# * hadoop_deploy: specifies whether to deploy hadoop or not
#   * hadoop_ha (hadoop high availability configuration):
#       enabled => hadoop ha is enabled with two namenode(s) with auto failover 
#                  using zkfc
#       disabled => hadoop ha is not setup and only one namenode will be 
#                   installed and configured
#   * hadoop_namenode:
#       fqdn of hosts on which namenode should be deployed
#       if hadoop_ha is enabled, pass in two fqdn(s)
#       if hadoop_ha is disabled, pass in one fqdn
#   * hadoop_secondarynamenode:
#       if hadoop_ha is disabled pass in a fqdn in which secondarynamenode is 
#       configured
#   * journal_quorum:
#       if hadoop_ha is enabled|auto, pass in odd number of fqdn(s). Journal 
#       nodes are used to store metadata in high availability configuration
#   * mapreduce: mapreduce specific configuration, specifies whether to deploy 
#                mapreduce framework or not
#       * type: specifies mapreduce framework type to deploy (mr1 or mr2)
#       * master: fqdn of machine on which mapreduce master will be deployed
#   * hadoop_ecosystem: list of ecosystem tools to deploy
#       hive  - installs and configures hive, which is data warehouse system for 
#               Hadoop (http://hive.apache.org/)
#       pig   - installs and configures pig, is a platform for analyzing large 
#               data sets (http://pig.apache.org/)
#       sqoop - installs and configures sqoop, is a tool designed for 
#               efficiently transferring bulk data between hadoop & rdbms & mpp
#               (http://sqoop.apache.org/)
#       oozie - installs and configures oozie, is a workflow scheduler system to
#               manage Apache Hadoop jobs 
#               (http://oozie.apache.org/)
#       hue   - installs and configures hue, the open source Apache Hadoop UI
#               (http://cloudera.github.io/hue/)
#
# ===============DEPLOYMENT SCENARIO 1==================
#        TO DEPLOY HADOOP (HA) with MapReduce
# ======================================================
# hadoop_deploy:
#   hadoop_ha: 'enabled'
#   hadoop_namenode:
#     - 'namenode001.ops.cw.com'
#     - 'namenode002.ops.cw.com'
#   journal_quorum:
#     - 'utils001.ops.cw.com'
#   mapreduce:
#     type: 'mr1'
#     master: 'jt.ops.cw.com'
#   hadoop_ecosystem:
#     - 'hive'
#     - 'pig'
#     - 'sqoop'
#     - 'oozie'
#
# ===============DEPLOYMENT SCENARIO 2==================
#       TO DEPLOY HADOOP (NON-HA) with MapReduce
# ======================================================
# hadoop_deploy:
#   hadoop_ha: 'disabled'
#   hadoop_namenode:
#     - 'namenode001.ops.cw.com'
#   hadoop_secondarynamenode: 'snn.ops.cw.com'
#   mapreduce:
#     type: 'mr1'
#     master: 'jt.ops.cw.com'
#   hadoop_ecosystem:
#     - 'hive'
#     - 'pig'
#     - 'sqoop'
#     - 'oozie'
#
# ===============DEPLOYMENT SCENARIO 3==================
# TO DEPLOY HADOOP (just HDFS) suitable for HBase deployments
# ======================================================
# hadoop_deploy:
#   hadoop_ha: 'disabled'
#   hadoop_namenode:
#     - 'namenode001.ops.cw.com'
#   hadoop_secondarynamenode: 'snn.ops.cw.com'
#   mapreduce: 'disabled'
#
# ===============DEPLOYMENT SCENARIO 4==================
#              DISABLE HADOOP DEPLOYMENT
# ======================================================
# hadoop_deploy: 'disabled'
#
########################################################
hadoop_deploy: 'disabled'

########################################################
# HBase Cluster configuration
########################################################
# * hbase_deploy: Specify whether to deploy hbase or not
#                   if enabled will install hbase
#                   if disabled hbase will not be deployed
# * hbase_master: if hbase_deploy is enabled, specify fqdn's of hbase masters 
#                 to deploy
#
# ===============DEPLOYMENT SCENARIO 1==================
#           TO DEPLOY HBASE (with 2 masters)
# ======================================================
# hbase_deploy:
#   hbase_master:
#     - 'hbasemaster001.ops.cw.com'
#     - 'hbasemaster002.ops.cw.com'
#
# ===============DEPLOYMENT SCENARIO 2==================
#               DISABLE HBASE DEPLOYMENT
# ======================================================
# hbase_deploy: 'disabled'
#
########################################################
hbase_deploy: 'disabled'

########################################################
# Zookeeper Quorum Configuration
########################################################
# Required for hadoop_ha, hbase, kafka, storm deployments
# * zookeeper_quoram: specifies the fqdn(s) of hosts to launch zookeeper, which
#                     are used for coordination between several services
#                     Recommended: Odd number of hosts
# Note: (zookeepers and journal nodes can co-exist on same machines)
########################################################
zookeeper_quorum:
  - 'utils001.ops.cw.com'

########################################################
# Worker Nodes Configuration
########################################################
#  slave_nodes:
#   list of slave nodes on which hadoop worker daemons will be installed. If 
#   hbase is enabled hbase worker daemons are also installed on same machines.
#  storage_dirs:
#   list of directories where worker nodes will store data, recommended to mount 
#   each dir on separate volumes for higher throughput
########################################################
slave_nodes:
  - 'worker001.ops.cw.com'
  - 'worker002.ops.cw.com'
  - 'worker003.ops.cw.com'
  - 'worker004.ops.cw.com'
  - 'worker005.ops.cw.com'

storage_dirs:
  - '/data/1'
  - '/data/2'
  - '/data/3'
  - '/data/4'

########################################################
# Solr Cluster configuration
########################################################
# * solr_deploy: Specify whether to deploy solr or not
#                   if enabled will install solr
#                   if disabled solr will not be deployed
# * hdfs_intergration: specifies whether to deploy solr with hdfs integration
#                      or not
# * solr_nodes: specify fqdn's of solr nodes to deploy, if hdfs_integration is
#               enabled its optimal to colocate hadoop worker nodes and solr 
#               nodes
#
# ===============DEPLOYMENT SCENARIO 1==================
#           TO DEPLOY Solr with HDFS Integration
# ======================================================
# solr_deploy:
#   hdfs_integration: enabled
#   solr_nodes:
#     - 'worker001.ops.cw.com'
#     - 'worker002.ops.cw.com'
#     - 'worker003.ops.cw.com'
#
# ===============DEPLOYMENT SCENARIO 1==================
#        TO DEPLOY Solr without HDFS Integration
# ======================================================
# solr_deploy:
#   hdfs_integration: disabled
#   solr_nodes:
#     - 'solr001.ops.cw.com'
#     - 'solr002.ops.cw.com'
#     - 'solr003.ops.cw.com'
#
# ===============DEPLOYMENT SCENARIO 2==================
#               DISABLE SOLR DEPLOYMENT
# ======================================================
# solr_deploy: 'disabled'
#
########################################################
solr_deploy: 'disabled'

########################################################
# Security
########################################################
# security              : if configured sets up kerberos and integrates it with
#                         hadoop/hbase and its eco-system
#                         possible_values - simple (will not install kerberos)
#                                         - kerberos (will install kerberos)
# hadoop_kerberos_realm : if kerberos is enabled, enter a kerberos realm name
# hadoop_kerberos_domain: if kerberos is enabled, enter a kerberos domain name
#                         (http://goo.gl/o44h1O)
#
# ===============DEPLOYMENT SCENARIO 1==================
#                DO NOT USE SECURITY
# ======================================================
#security: 'simple'
#
# ===============DEPLOYMENT SCENARIO 2==================
#          ENABLE SECURITY USING KERBEROS
# ======================================================
#security: 'kerberos'
# hadoop_kerberos_realm: ANKUS.COM
# hadoop_kerberos_domain: ankus.com
#
########################################################
security: 'simple'

########################################################
# Cassandra Cluster Configuration
#  cassandra_deploy: specifies the deployment of cassandra cluster
#                    possible values: Hash (or) disabled
#   cassandra_nodes: nodes on which cassandra is deployed and managed
#   cassandra_seeds: list of cassandra seed nodes
# (NOTE: cassandra_nodes and hadoop_slave_nodes can co-exist if willing to 
#        sacrifice performance)
#
# ===============DEPLOYMENT SCENARIO 1==================
#                 TO DEPLOY CASSANDRA 
# ======================================================
# cassandra_deploy:
#   cassandra_nodes:
#     - 'cassandra001.ops.cw.com'
#     - 'cassandra002.ops.cw.com'
#     - 'cassandra003.ops.cw.com'
#     - 'cassandra004.ops.cw.com'
#     - 'cassandra005.ops.cw.com'
#   cassandra_seeds:
#     - 'cassandra002.ops.cw.com'
#
# ===============DEPLOYMENT SCENARIO 2==================
#           DISABLE CASSANDRA DEPLOYMENT
# ======================================================
#cassandra_deploy: 'disabled'
#
########################################################
cassandra_deploy: 'disabled'

########################################################
# Kafka Cluster Configuration
#  kafka_deploy: specifies the deployment of kafka cluster
#                possible values: Hash (or) disabled
#   kafka_nodes: nodes on which kafka package is installed (can be used as
#                 producers and consumenrs)
#   kafka_brokers: list of kafka broker nodes
#
# ===============DEPLOYMENT SCENARIO 1==================
#                   TO DEPLOY KAFKA
# ======================================================
# kafka_deploy:
#   kafka_brokers:
#     - 'kafka001.ops.cw.com'
#     - 'kafka002.ops.cw.com'
#
# ===============DEPLOYMENT SCENARIO 2==================
#               DISABLE KAFKA DEPLOYMENT
# ======================================================
#kafka_deploy: 'disabled'
#
########################################################
kafka_deploy: 'disabled'

########################################################
# Storm Cluster Configuration
#  storm_deploy: specifies the deployment of storm cluster
#                possible values: Hash (or) disabled
#   storm_supervisors: nodes on which storm supervisor daemons are installed
#   storm_master: node on which storm nimbus and ui daemons are installed
#   workers_count: number of worker processes to run per supervisor node
#
# ===============DEPLOYMENT SCENARIO 1==================
#                   TO DEPLOY STORM
# ======================================================
# storm_deploy:
#   storm_supervisors:
#     - 'stormsupervisor001.ops.cw.com'
#     - 'stormsupervisor002.ops.cw.com'
#     - 'stormsupervisor003.ops.cw.com'
#     - 'stormsupervisor004.ops.cw.com'
#     - 'stormsupervisor005.ops.cw.com'
#   storm_master: 'stormmaster001.ops.cw.com'
#   workers_count: 8
#
# ===============DEPLOYMENT SCENARIO 2==================
#               DISABLE STORM DEPLOYMENT
# ======================================================
# storm_deploy: 'disabled'
#
########################################################
storm_deploy: 'disabled'

########################################################
# Management Configuration
########################################################
# monitoring      : whether to enable monitoring if the instances using ganglia
#                   possible values: enabled,disabled
# alerting        : whether to monitor services and send alerts if something
#                   goes wrong based on nagios
#                   possible values: enabled,disabled
#   admin_email   : email address to send alerts (notifications)
# log_aggregation : whether to collect logs from all the services for core
#                   services and make them available through web console
#                   possible values: enabled,disabled
########################################################
monitoring: 'disabled'
alerting: 'disabled'
admin_email: ''                   #<<<<<<<<<<<<<< FILL THIS
log_aggregation: 'disabled'