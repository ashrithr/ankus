---
########################################################
#             Ankus Local Configuration
########################################################

########################################################
# * install_mode: Specifies the type of installation
#     Possible values: local | cloud
#     # local: installation will be done on pre-defined machines (managed by user)
#     # cloud: ankus will create machines in the cloud (managed by ankus)
########################################################
install_mode: 'local'

########################################################
# SSH Configuration
# * ssh_key: root users ssh private key to log into machines
# * ssh_user: username used to log into machines, change this if the user is not 'root', ssh_user
#             should have password less sudo privileges (you can make a user password-less sudo by adding the user
#             to /etc/sudoers file like this `ssh_user  ALL = (ALL) NOPASSWD: ALL` )
########################################################
ssh_key: '~/.ssh/ruby-sample-1363113606'
ssh_user: 'root'

########################################################
# Controller
# * controller: hostname of the server which will host puppet master, kerberos server
#   if security is enabled, nagios & ganglia servers if monitoring & altering are enabled.
#   logstash indexer if log_aggregation is enabled this could be local host if running
#   ankuscli from controller machine.
########################################################
controller: 'controller.ops.cw.com'

########################################################
# Hadoop Cluster Configuration
# * hadoop_deploy: specifies whether to deploy hadoop or not
#   * hadoop_ha (hadoop high availability configuration):
#       enabled => hadoop ha is enabled with two namenode(s) with auto failover using zkfc
#       disabled => hadoop ha is not setup and only one namenode will be installed and configured
#   * hadoop_namenode:
#       fqdn of hosts on which namenode should be deployed
#       if hadoop_ha is enabled, pass in two fqdn(s)
#       if hadoop_ha is disabled, pass in one fqdn
#   * hadoop_secondarynamenode:
#       if hadoop_ha is disabled pass in a fqdn in which secondarynamenode is configured
#   * journal_quorum:
#       if hadoop_ha is enabled|auto, pass in odd number of fqdn(s). Journal nodes are used to store
#       metadata in high availability configuration
#   * mapreduce: mapreduce specific configuration, specifies whether to deploy mapreduce framework or not
#       * type: specifies mapreduce framework type to deploy (mr1 or mr2)
#       * master: fqdn of machine on which mapreduce master will be deployed
#   * hadoop_ecosystem: list of ecosystem tools to deploy
#       hive  - installs and configures hive, which is data warehouse system for Hadoop (http://hive.apache.org/)
#       pig   - installs and configures pig, is a platform for analyzing large data sets (http://pig.apache.org/)
#       sqoop - installs and configures sqoop, is a tool designed for efficiently transferring bulk data between
#               hadoop & rdbms & mpp (http://sqoop.apache.org/)
#       oozie - installs and configures oozie, is a workflow scheduler system to manage Apache Hadoop jobs
#               (http://oozie.apache.org/)
#       hue   - installs and configures hue, the open source Apache Hadoop UI (http://cloudera.github.io/hue/)
#
#  ======================================================
# Example Configurations:
# 1. To deploy hadoop (ha) with mapreduce
# hadoop_deploy:
#   hadoop_ha: 'enabled'
#   hadoop_namenode:
#     - 'namenode001.ops.cw.com'
#     - 'namenode002.ops.cw.com'
#   journal_quorum:
#     - 'utils001.ops.cw.com'
#   mapreduce:
#     type: mr1
#     master: 'jt.ops.cw.com'
#   hadoop_ecosystem:
#     - hive
#     - pig
#     - sqoop
#     - oozie
#
# 2. To deploy only hadoop(non-ha), used for hbase oriented deployments
# hadoop_deploy:
#   hadoop_ha: 'disabled'
#   hadoop_namenode:
#     - 'namenode001.ops.cw.com'
#   mapreduce: disabled
#
# 3. Not to deploy hadoop, used for cassandra oriented deployments
# hadoop_deploy: 'disabled'
########################################################
hadoop_deploy:
  hadoop_ha: 'enabled'
  hadoop_namenode:
    - 'namenode001.ops.cw.com'
    - 'namenode002.ops.cw.com'
  journal_quorum:
    - 'utils001.ops.cw.com'
  #hadoop_secondarynamenode: 'snn.ops.cw.com'
  mapreduce:
    type: mr1
    master: 'jobtracker.ops.cw.com'
  hadoop_ecosystem:
    - hive
    - sqoop
    - pig

########################################################
# HBase Cluster configuration
# * hbase_install: Specify whether to deploy hbase or not
#     if enabled will install hbase
#     if disabled hbase will not be deployed
# * hbase_master: if hbase_install is enabled, specify fqdn's of hbase masters to deploy
#
#  ======================================================
# Example configurations
# 1. Deploy hbase on 2 masters
# hbase_deploy:
#   hbase_master:
#     - 'hbasemaster001.ops.cw.com'
#     - 'hbasemaster002.ops.cw.com'
#
# 2. Do noy deploy hbase
# hbase_install: disabled
########################################################
hbase_deploy:
  hbase_master:
    - 'hbasemaster001.ops.cw.com'
    - 'hbasemaster002.ops.cw.com'

########################################################
# Zookeeper Quorum Configuration (Required when hadoop_ha or hbase is enabled)
# * zookeeper_quoram_count: specifies the number of zookeeper(s) to launch used for coordination
#                           services of hadoop-ha and hbase. Recommended is Odd number of zookeepers.
# Note: (zookeepers and journal nodes can co-exist on same machines)
########################################################
zookeeper_quorum:
  - 'utils001.ops.cw.com'

########################################################
# Worker Nodes Configuration
#  list of slave nodes on which datanode and tasktracker daemons will be installed. If hbase is enabled
#  region servers are also installed on same instances.
########################################################
slave_nodes:
  - 'worker001.ops.cw.com'
  - 'worker002.ops.cw.com'
  - 'worker003.ops.cw.com'
  - 'worker004.ops.cw.com'
  - 'worker005.ops.cw.com'

########################################################
# Security Config
# security => if configured sets up kerberos and integrates it with hadoop/hbase and its eco-system
#   possible_values: simple => will not install kerberos
#                    kerberos => will install kerberos
# hadoop_kerberos_realm => if kerberos is enabled, enter a kerberos realm name
# hadoop_kerberos_domain => if kerberos is enabled, enter a kerberos domain name
#   (http://web.mit.edu/kerberos/krb5-1.5/krb5-1.5/doc/krb5-admin/domain_realm.html)
########################################################
security: simple
# if security is kerberos please fill in the below options
#hadoop_kerberos_realm: CW.COM
#hadoop_kerberos_domain: cw.com

########################################################
# Cassandra Cluster Configuration
#  cassandra_deploy: specifies the deployment of cassandra cluster
#                    possible values: Hash (or) disabled
#  hadoop_collocation: whether to collocate hadoop slave daemons and cassandra daemons
#  number_of_instances: number of cassandra instances to launch(not required when
#                       collocation instances)
# (NOTE: cassandra_nodes and hadoop_slave_nodes can co-exist if willing to sacrifice performance)
#
# Example Configurations:
# 1. To deploy cassandra on set of nodes
# cassandra_deploy:
#   cassandra_nodes:
#     - 'cassandra001.ops.cw.com'
#     - 'cassandra001.ops.cw.com'
#     - 'cassandra001.ops.cw.com'
#
# 2. Not to deploy cassandra
# cassandra_deploy: 'disabled'
########################################################
cassandra_deploy: 'disabled'

########################################################
# Management Configuration
# monitoring (enabled|disabled) => if enabled, ganglia will be setup and configured
# alerting (enabled|disabled) => if enabled, nagios will be setup and configured to monitor
# admin_email => required (if alerting is enabled), email address to send alters (notifications)
# log_aggregation (enabled|disabled) => if enabled, logstash will be installed to aggregate logs from all the servers
########################################################
monitoring: enabled
alerting: enabled
admin_email: 'admin@cw.com'
log_aggregation: enabled